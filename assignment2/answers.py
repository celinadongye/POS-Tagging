a1a=['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']
a1b=2649
a1c=12.059885650136557
a1d='function'
a2a=13
a2b=2.4630453700815003
a4a3=0.8689827219809665
a4b1=[("I'm", 'PRT'), ('useless', 'ADJ'), ('for', 'ADP'), ('anything', 'NOUN'), ('but', 'CONJ'), ('racing', 'ADJ'), ('cars', 'NOUN'), ('.', '.')]
a4b2=[("I'm", 'PRT'), ('useless', 'ADJ'), ('for', 'ADP'), ('anything', 'NOUN'), ('but', 'ADP'), ('racing', 'VERB'), ('cars', 'NOUN'), ('.', '.')]
a4b3="'but' is tagged as conjunction since this word usually acts as a conjunction but \nadposition is the right tag as it means 'except for'.\n'racing' was tagged as an adjective attached to 'cars' to denote the type of cars, while\nthe 'correct' tagging is verb, representing an action."
a4c=56.63057530953183
a4d=308.7122785474796
a4e=['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'ADV']
a5='Have a dataset of synonyms and if we encounter a word we have not seen before\nwe can map to its synonym and tag it correspondingly. That is, we could consider\nthe semantics of the unseen word to tag it.\n\nSmoothing for zero probabilities (tags that have zero probabilities)???'
a6='The Brown corpus has significantly more tags than the Universal tagset.\nWith the same training dataset, we would not have enough words that are\nrepresentative of each possible tag. That is, the distribution of the words\nfor each tag would have been really spread out. Thus, a lot of probabilities\nwould be really small (if not 0) differing only slightly. Consequently, the\nmodel would be more prone to mistagging words, and would be more inaccurate.\nThus, more training data would be needed.'
a3c=16.79319240474419
a3d=0
