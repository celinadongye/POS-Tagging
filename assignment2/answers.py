a1a=['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']
a1b=2649
a1c=12.059885650136557
a1d='function'
a2a=13
a2b=2.4630453700815003
a4a3=0.8689827219809665
a4b1=[("I'm", 'PRT'), ('useless', 'ADJ'), ('for', 'ADP'), ('anything', 'NOUN'), ('but', 'CONJ'), ('racing', 'ADJ'), ('cars', 'NOUN'), ('.', '.')]
a4b2=[("I'm", 'PRT'), ('useless', 'ADJ'), ('for', 'ADP'), ('anything', 'NOUN'), ('but', 'ADP'), ('racing', 'VERB'), ('cars', 'NOUN'), ('.', '.')]
a4b3="'but' is tagged as conjunction since this word usually acts as a conjunction but \nadposition is the right tag as it means 'except for'.\n'racing' was tagged as an adjective attached to 'cars' to denote the type of cars, while\nthe 'correct' tagging is verb, representing an action."
a4c=56.63057530953183
a4d=308.7122785474796
a4e=['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'ADV']
a5='The tagger can be used as a filter to select the most probable tags out\nof the possible sequences of tags produced by the parser, which would reduce\nthe number of possible lexical categories by removing unlikely tags. But, this\napproach does not always do better than the original parser because for words\nthat have previously been seen, the most likely tagging sequence generated by\nthe parser might be correct, and by filtering it through the tagger, it might\nget an incorrect tag assigned.'
a6='The Brown corpus has significantly more tags than the Universal tagset.\nWith the same training dataset, we would not have enough words that are\nrepresentative of each possible tag. That is, the distribution of the words\nfor each tag would have been really spread out. Thus, a lot of probabilities\nwould be really small (if not 0) differing only slightly. Consequently, the\nmodel would be more prone to mistagging words, and would be more inaccurate.\nThus, more training data would be needed.'
a3c=16.79319240474419
a3d=0
