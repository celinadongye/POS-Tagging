a1a=['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']
a1b=2649
a1c=12.059885650136557
a1d='function'
a2a=13
a2b=2.4630453700815003
a4a3=0.8689827219809665
a4b1=[("I'm", 'PRT'), ('useless', 'ADJ'), ('for', 'ADP'), ('anything', 'NOUN'), ('but', 'CONJ'), ('racing', 'ADJ'), ('cars', 'NOUN'), ('.', '.')]
a4b2=[("I'm", 'PRT'), ('useless', 'ADJ'), ('for', 'ADP'), ('anything', 'NOUN'), ('but', 'ADP'), ('racing', 'VERB'), ('cars', 'NOUN'), ('.', '.')]
a4b3="Some sentences are ambiguous in terms of attachments and syntactic meaning.\nThe tagging of the sentence above seems correct in both cases, although they\nboth have a different meaning. The tagging by our model is has 'racing' as an\nadjective attached to 'cars' in order to denote t"
a4c=56.63057530953183
a4d=308.7122785474796
a4e=['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'ADV']
a5='Semantics synonyms????\nHave a dataset of synonyms and if we encounter a word we have not seen before\nwe can map to its synonym and tag it correspondingly.\n\nKnow the probability of a NN following a DET, so if we see a DET and we have not\nseen the word following it, we will just tag it with the most probable tag.'
a6='The Universal tagset has 17 tags, whereas the Brown corpus has 86 tags, significantly\nmore than the Universal tagset. This means, that the tagging by our model would have\nbeen quite inaccurate due to the fact the distribution of the the words for each tag \nwould have been really spread out. That is, with the same training dataset,\nwe would not have had enough words for each tag, and thus, the probabilities of each\ntag would only differ slightly (the difference would not be very large), and the m'
a3c=16.79319240474419
a3d=0
